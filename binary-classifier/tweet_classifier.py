# -*- coding: utf-8 -*-
"""Tweet Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-jPdoOwz1JLchYNyRIaHtFykNOy6CaJJ
"""

!pip install transformers

import numpy as np
import pandas as pd

from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
import torch

from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tqdm
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
import random
import os
import json

VALID_SIZE = 0.1
EPOCHS = 10
TRAIN_BATCH_SIZE = 32
VALID_BATCH_SIZE = 32
MAX_LEN = 64
SEED_VALUE = 42

with open('merged.json') as input_file:
  data = json.load(input_file)

len(data)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased", do_lower_case=True)

if torch.cuda.is_available():
  device = torch.device("cuda:0")
  print("Using gpu")
else:
  device = torch.device("cpu")
  print("Using Cpu")

train_data, valid_data = train_test_split(data)

train_texts = [d["text"] for d in train_data]
train_labels = [d["label"].lower()=="true" for d in train_data]

valid_texts = [d["text"] for d in valid_data]
valid_labels = [d["label"].lower()=="true" for d in valid_data]

train_input_ids = [None for i in range(len(train_texts))]
for i in tqdm.trange(len(train_texts)):
  sent = train_texts[i]
  encoded = tokenizer.encode(sent, add_special_tokens=True)
  train_input_ids[i] = encoded

valid_input_ids = [None for i in range(len(valid_texts))]
for i in tqdm.trange(len(valid_texts)):
  sent = valid_texts[i]
  encoded = tokenizer.encode(sent, add_special_tokens=True)
  valid_input_ids[i] = encoded

train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype="long", value=0, truncating="post", padding="post")
valid_input_ids = pad_sequences(valid_input_ids, maxlen=MAX_LEN, dtype="long", value=0, truncating="post", padding="post")

train_attention_masks = [None for i in range(len(train_input_ids))]
for i in tqdm.trange(len(train_input_ids)):
  sent = train_input_ids[i]
  att_mask = [int(token_id > 0) for token_id in sent]
  train_attention_masks[i] = att_mask

valid_attention_masks = [None for i in range(len(valid_input_ids))]
for i in tqdm.trange(len(valid_input_ids)):
  sent = valid_input_ids[i]
  att_mask = [int(token_id > 0) for token_id in sent]
  valid_attention_masks[i] = att_mask

train_inputs = torch.tensor(train_input_ids)
valid_inputs = torch.tensor(valid_input_ids)
train_labels = torch.tensor(train_labels).long()
valid_labels = torch.tensor(valid_labels).long()
train_masks = torch.tensor(train_attention_masks)
valid_masks = torch.tensor(valid_attention_masks)

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = TRAIN_BATCH_SIZE)

valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)
valid_sampler = RandomSampler(valid_data)
valid_dataloader = DataLoader(valid_data, sampler = valid_sampler, batch_size = VALID_BATCH_SIZE)

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2, output_attentions=False, output_hidden_states = False)

model = model.to(device)

optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)
total_steps = len(train_dataloader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

def accuracy(preds, labels):
  
  pred_flatten = np.argmax(preds, axis=-1).flatten().astype("int")
  labels_flatten = labels.flatten().astype("int")
  
  return (pred_flatten==labels_flatten).astype("float").sum()/len(labels_flatten)

random.seed(SEED_VALUE)
np.random.seed(SEED_VALUE)
torch.manual_seed(SEED_VALUE)
torch.cuda.manual_seed_all(SEED_VALUE)

def train_one_epochs(model, loader, optimizer):
  model.train()
  ema_loss = None
  t = tqdm.notebook.tqdm(loader)
  losses = []
  for  inputs, masks, labels in t:
      inputs = inputs.to(device)
      masks = masks.to(device)
      labels = labels.to(device)

      outputs = model(inputs, token_type_ids=None, attention_mask=masks, labels = labels)
      
      loss = outputs[0]
      losses.append(loss.item())
      optimizer.zero_grad()
      if ema_loss is None:
        ema_loss = loss.item()
      else:
        ema_loss += 0.1 * (loss.item() - ema_loss)
      t.set_description("loss: {:.4f}".format(ema_loss))
      t.refresh()
      loss.backward()
      optimizer.step()
  return np.mean(losses)

def test_model(model, loader):
  model.eval()
  with torch.no_grad():
    accs = []
    t = tqdm.notebook.tqdm(loader)
    for  inputs, masks, labels in t:
        inputs = inputs.to(device)
        masks = masks.to(device)
        

        outputs = model(inputs, token_type_ids=None, attention_mask=masks)
        logits = outputs[0]
        logits = logits.detach().cpu().numpy()
        label_ids = labels.numpy()
        batch_acc = accuracy(logits, label_ids)
        accs.append(batch_acc)
        val_acc = np.mean(accs)
        t.set_description("val_acc: {:.4f}".format(val_acc))
        t.refresh()
    return val_acc

def train_loop(model, train_loader, valid_loader, optimizer):
  losses = []
  valid_accs = []
  t = tqdm.notebook.trange(EPOCHS)
  prev_val_accuracy = 0
  for epoch in t:
    loss = train_one_epochs(model, train_loader, optimizer)
    losses.append(loss)
    valid_acc = test_model(model, valid_loader)
    if valid_acc > prev_val_accuracy:
      torch.save(model.state_dict(), 'best_model-{}-{:.2f}.pt'.format(epoch+1, valid_acc * 100))
      prev_val_accuracy = valid_acc

    valid_accs.append(valid_acc)
    
  return losses, valid_accs

losses, valid_accs = train_loop(model, train_dataloader, valid_dataloader, optimizer)



